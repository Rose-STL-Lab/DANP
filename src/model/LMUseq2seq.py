import torch
from torch import nn
from model.lmu import LMUCell


class EncoderLMU(nn.Module):
    def __init__(self, num_layers, input_size, hidden_size, memory_size, theta, device):
        super(EncoderLMU, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.memory_size = memory_size
        self.theta = theta

        # self.fc  = nn.Linear(input_size, hidden_size)
        self.fc = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
        ).to(device)

        self.lmu_layers = nn.ModuleList()
        self.num_layers = num_layers

        for _ in range(num_layers):
            self.lmu_layers.append(
                LMUCell(
                    input_size=hidden_size,
                    hidden_size=hidden_size,
                    memory_size=memory_size,
                    theta=theta,
                )
            )

        self.device = device
        self.to(device)

    # x : [batch_size, seq_len, input_size]
    def forward(self, x, state=None):
        input = self.fc(x)
        output = []

        # Assuming batch dimension is always first, followed by seq. length as the second dimension
        batch_size = x.size(0)
        seq_len = x.size(1)

        # Initial state (h_0, m_0)
        if state is None:
            h0 = torch.zeros(batch_size, self.hidden_size).to(self.device)
            m0 = torch.zeros(batch_size, self.memory_size).to(self.device)
            state = (h0, m0)

        # Iterate over the timesteps
        output = []
        for t in range(seq_len):
            input_t = input[:, t, :]  # [batch_size, hidden_size]
            for cell in self.lmu_layers:
                h_t, m_t = cell(input_t, state)
                state = (h_t, m_t)
            output.append(h_t)

        output = torch.stack(output, 1)  # [batch_size, seq_len, hidden_size]

        return output


class DecoderLMU(nn.Module):
    def __init__(
        self, num_layers, seq_len, output_size, hidden_size, memory_size, theta, device
    ):
        super(DecoderLMU, self).__init__()
        self.seq_len = seq_len

        self.output_size = output_size
        self.hidden_size = hidden_size
        self.memory_size = memory_size
        self.theta = theta

        # self.fc  = nn.Linear(hidden_size, output_size)
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size),
        ).to(device)

        self.lmu_layers = nn.ModuleList()
        self.num_layers = num_layers

        for _ in range(num_layers):
            self.lmu_layers.append(
                LMUCell(
                    input_size=hidden_size,
                    hidden_size=hidden_size,
                    memory_size=memory_size,
                    theta=theta,
                )
            )

        self.device = device
        self.to(device)

    # x is generated by EncoderLMU, [batch_size, hidden_size]
    def forward(self, x, state=None):
        output = []

        # Assuming batch dimension is always first
        batch_size = x.size(0)

        # Initial state (h_0, m_0)
        if state is None:
            h0 = torch.zeros(batch_size, self.hidden_size).to(self.device)
            m0 = torch.zeros(batch_size, self.memory_size).to(self.device)
            state = (h0, m0)

        # Iterate over the timesteps
        output = []
        input = x
        for _ in range(self.seq_len):
            for cell in self.lmu_layers:
                h_t, m_t = cell(input, state)
                state = (h_t, m_t)
            input = h_t
            output.append(h_t)

        output = torch.stack(output, 1)  # [seq_len, batch_size, hidden_size]
        output = self.fc(output)

        return output


class DecoderLMUAttn(nn.Module):
    def __init__(
        self, num_layers, seq_len, output_size, hidden_size, memory_size, theta, device
    ):
        super(DecoderLMUAttn, self).__init__()
        self.seq_len = seq_len

        self.output_size = output_size
        self.hidden_size = hidden_size
        self.memory_size = memory_size
        self.theta = theta

        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size),
        ).to(device)

        # bilinear attn function
        self.bil = nn.Parameter(torch.zeros(hidden_size, hidden_size))

        self.lmu_layers = nn.ModuleList()
        self.num_layers = num_layers

        for _ in range(num_layers):
            self.lmu_layers.append(
                LMUCell(
                    input_size=hidden_size,
                    hidden_size=hidden_size,
                    memory_size=memory_size,
                    theta=theta,
                )
            )

        self.device = device
        self.to(device)

    # x is generated by EncoderLMU, [batch_size, inp_len, hidden_size]
    def forward(self, x, state=None):
        output = []

        # Assuming batch dimension is always first
        batch_size = x.size(0)

        # Initial state (h_0, m_0)
        if state is None:
            h0 = torch.zeros(batch_size, self.hidden_size).to(self.device)
            m0 = torch.zeros(batch_size, self.memory_size).to(self.device)
            state = (h0, m0)
        else:
            (h0, m0) = state

        # Iterate over the timesteps
        output = []
        score = torch.softmax(x.matmul(self.bil).bmm(h0.unsqueeze(-1)), dim=1)
        input = torch.sum(x * score, 1)

        for _ in range(self.seq_len):
            for cell in self.lmu_layers:
                h_t, m_t = cell(input, state)
                state = (h_t, m_t)
            score = torch.softmax(x.matmul(self.bil).bmm(h_t.unsqueeze(-1)), dim=1)
            input = torch.sum(x * score, 1)
            output.append(h_t)

        output = torch.stack(output, 1)  # [seq_len, batch_size, hidden_size]
        output = self.fc(output)

        return output


class LMUSeq2Seq(nn.Module):
    def __init__(
        self,
        num_layers,
        seq_len,
        input_size,
        output_size,
        hidden_size,
        memory_size,
        theta,
        device,
        attn=False,
    ):
        super(LMUSeq2Seq, self).__init__()
        self.seq_len = seq_len

        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size = hidden_size
        self.memory_size = memory_size
        self.theta = theta
        self.device = device

        self.enc = EncoderLMU(
            num_layers, input_size, hidden_size, memory_size, theta, device
        )
        self.attn = attn
        if attn:
            self.dec = DecoderLMUAttn(
                num_layers,
                seq_len,
                output_size,
                hidden_size,
                memory_size,
                theta,
                device,
            )
        else:
            self.dec = DecoderLMU(
                num_layers,
                seq_len,
                output_size,
                hidden_size,
                memory_size,
                theta,
                device,
            )

    # x : [batch_size, seq_len, input_size]
    def forward(self, x):
        s = self.enc(x.to(self.device))
        if self.attn:
            output = self.dec(s)  # [batch_size, output_seq_len, output_size]
        else:
            output = self.dec(s[:, -1, :])  # Only take the last step output
        if output.shape[-1] == 1:
            output = output.squeeze(-1)
        return output


class CondLMUSeq2Seq(nn.Module):
    """
    cond: shape (2, 4)
    feature std, first row ami-cgs, second row hrpci
    """

    def __init__(
        self,
        num_layers,
        seq_len,
        cond,
        input_size,
        output_size,
        hidden_size,
        memory_size,
        theta,
        device,
        attn=False,
        perturb="uniform",
        encode_only=False,
    ):
        super(CondLMUSeq2Seq, self).__init__()
        self.seq_len = seq_len

        self.cond = cond.to(device)
        self.cond_size = cond.shape[-1]

        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size = hidden_size
        self.memory_size = memory_size
        self.theta = theta
        self.device = device
        self.perturb = perturb
        self.encode_only = encode_only

        self.enc = EncoderLMU(
            num_layers, input_size, hidden_size, memory_size, theta, device
        )
        self.attn = attn
        if attn:
            self.dec = DecoderLMUAttn(
                num_layers,
                seq_len,
                output_size,
                hidden_size,
                memory_size,
                theta,
                device,
            )
        else:
            self.dec = DecoderLMU(
                num_layers,
                seq_len,
                output_size,
                hidden_size,
                memory_size,
                theta,
                device,
            )

        self.cen = nn.Sequential(  # Condition encoder
            nn.Linear(self.cond_size, hidden_size),
            nn.Tanh(),
            nn.Dropout(),
            nn.Linear(self.hidden_size, hidden_size),
            nn.Tanh(),
            nn.Dropout(),
            nn.Linear(hidden_size, 2 * hidden_size),
        ).to(device)

    # Use each sequence's feature average as a time-invariant condition
    # x : [batch_size, seq_len, input_size]
    def forward(self, x, labels):
        labels = labels[:, -2:]  # last 2 column: A or H

        scale = torch.std(self.cond, 0)
        cond = labels.mm(self.cond)
        if self.training:
            if self.perturb == "uniform":
                cond += (
                    (torch.rand(len(x), self.cond.shape[-1]).to(self.device) - 1) * 0.3 * scale
                )
            elif self.perturb == "normal":
                cond += torch.normal(torch.zeros_like(scale), 0.1 * scale).to(
                    self.device
                )

        state = torch.split(
            self.cen(cond), [self.hidden_size, self.hidden_size], dim=-1
        )
        s = self.enc(x.to(self.device), state)

        if self.encode_only:
            output = s
        else:
            if self.attn:
                output = self.dec(s, state)  # [batch_size, output_seq_len, output_size]
            else:
                output = self.dec(s[:, -1, :], state)  # Only take the last step output

        if output.shape[-1] == 1:
            output = output.squeeze(-1)
        return output
