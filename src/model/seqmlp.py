import torch
from torch import nn
from model.LMUseq2seq import EncoderLMU, DecoderLMU, DecoderLMUAttn
from model.LSTMseq2seq import EncoderLSTM


class DecoderMLP(nn.Module):
    def __init__(self, seq_len, output_size, hidden_size, device):
        super(DecoderMLP, self).__init__()
        self.seq_len = seq_len

        self.output_size = output_size
        self.hidden_size = hidden_size

        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, seq_len * output_size),
        ).to(device)

        self.device = device
        self.to(device)

    # x is generated by Encoder, [batch_size, hidden_size]
    def forward(self, x):
        return self.fc(x).view(-1, self.seq_len, self.output_size)


class LMUSeq2Seq(nn.Module):
    """
    cond: shape (2, 4)
    feature std, first row ami-cgs, second row hrpci
    """

    def __init__(
        self,
        num_layers,
        seq_len,
        input_size,
        output_size,
        hidden_size,
        memory_size,
        theta,
        device,
        attn=False,
    ):
        super(LMUSeq2Seq, self).__init__()
        self.seq_len = seq_len

        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size = hidden_size
        self.memory_size = memory_size
        self.theta = theta
        self.device = device

        self.enc = EncoderLMU(
            num_layers, input_size, hidden_size, memory_size, theta, device
        )
        self.attn = attn
        if attn:
            self.dec = DecoderLMUAttn(
                num_layers,
                seq_len,
                output_size,
                hidden_size,
                memory_size,
                theta,
                device,
            )
        else:
            self.dec = DecoderLMU(
                num_layers,
                seq_len,
                output_size,
                hidden_size,
                memory_size,
                theta,
                device,
            )

    # x : [batch_size, seq_len, input_size]
    def forward(self, x):
        s = self.enc(x.to(self.device))
        if self.attn:
            output = self.dec(s)  # [batch_size, output_seq_len, output_size]
        else:
            output = self.dec(s[:, -1, :])  # Only take the last step output
        if output.shape[-1] == 1:
            output = output.squeeze(-1)
        return output


class LMUMLP(nn.Module):
    def __init__(
        self,
        num_layers,
        seq_len,
        input_size,
        output_size,
        hidden_size,
        memory_size,
        theta,
        device,
    ):
        super(LMUMLP, self).__init__()
        self.seq_len = seq_len

        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size = hidden_size
        self.memory_size = memory_size
        self.theta = theta
        self.device = device

        self.enc = EncoderLMU(
            num_layers, input_size, hidden_size, memory_size, theta, device
        )
        self.dec = DecoderMLP(seq_len, output_size, hidden_size, device)

    def forward(self, x):
        s = self.enc(x.to(self.device))
        return self.dec(s[:, -1, :])


class LSTMMLP(nn.Module):
    def __init__(
        self, num_layers, seq_len, input_size, output_size, hidden_size, device
    ):
        super(LSTMMLP, self).__init__()
        self.seq_len = seq_len

        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size = hidden_size
        self.device = device

        self.enc = EncoderLSTM(num_layers, input_size, hidden_size, device)
        self.dec = DecoderMLP(seq_len, output_size, hidden_size, device)

    def forward(self, x):
        s = self.enc(x.to(self.device))
        return self.dec(s[:, -1, :])
